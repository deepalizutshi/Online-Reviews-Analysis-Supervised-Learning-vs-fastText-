{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751b89aa-159f-49e2-bc5f-16fa61ae9e79",
   "metadata": {},
   "source": [
    "## Create Spark Session and read data into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564e816f-d19a-4d1c-981e-a8bf02b063b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/deepali/Desktop/DATA -228-22/PROJECT/CODE/data/Software.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9fd48fac5bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   ])\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Software.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/deepali/Desktop/DATA -228-22/PROJECT/CODE/data/Software.json"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "      StructField(\"reviewerID\",StringType(),True),\n",
    "      StructField(\"asin\",StringType(),True),\n",
    "      StructField(\"reviewerName\",StringType(),True),\n",
    "      StructField(\"helpful\",StringType(),True),\n",
    "      StructField(\"reviewText\",StringType(),True),\n",
    "      StructField(\"overall\",StringType(),True),\n",
    "      StructField(\"summary\",StringType(),True),\n",
    "      StructField(\"unixReviewTime\",StringType(),True),\n",
    "      StructField(\"reviewTime\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "df = spark.read.schema(schema).json('data/Software.json')\n",
    "df.show(5,truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3fe02-571d-46a5-b90c-551be8298c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import rand \n",
    "\n",
    "df = df.selectExpr(\"cast(reviewText as string) reviewText\",\n",
    "                    \"cast(overall as int) overall\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e91cf-6e10-474b-80f3-1daaab80f415",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fccac-5a39-4d91-afba-92c0b0b4dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(\"any\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8460d1f-e1b8-4e53-a5c5-7bdaa19263ee",
   "metadata": {},
   "source": [
    "## Create a table from data to work with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b59675-3fbe-4256-96f1-bd1e3d3ed667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "# creating sparksession and giving app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaebf12-a23a-4112-b07a-b8f15ba973f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering against review scores more than 5 or less than 1\n",
    "df = df.filter(\"overall<6 and overall!=3\")\n",
    "df = df.filter(\"overall>0\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4f879-c2b0-403b-aef4-39fbf90fdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "# map review scores into two categories\n",
    "bucketizer = Bucketizer(splits=[ 1, 4, 5 ],inputCol=\"overall\", outputCol=\"label\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c32960-49be-422f-818a-eff7b8b6805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "# creating sparksession and giving app name\n",
    "\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd8552-3e06-4b62-9622-adad139079ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc19a5-0c8a-443b-a7ec-b4bf60876723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping reviewText and label column\n",
    "df = df[\"reviewText\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90d660-fa3b-44ea-8b0c-279509141d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling rows in df\n",
    "df = df.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7d841-80cb-4c75-aee0-2d3cca08deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how data is spread among two categories\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((label)),label from df group by label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078c672-cb70-49a2-a0ca-108513d8ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c37d0-ce87-45ae-a31a-23f08d70b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sq\n",
    "from pyspark.sql.functions import lower, col\n",
    "#replace regex\n",
    "df = df.select(\"*\", lower(col('reviewText')).alias(\"lower_text\"))\n",
    "df = df.withColumn(\"no_line_text\", sq.regexp_replace(\"lower_text\", r\"\\n\", \" \"))\n",
    "df = df.withColumn(\"no_digit_text\", sq.regexp_replace(\"no_line_text\", r\"[0-9]\", \" \"))\n",
    "df = df.withColumn(\"text_ready\", sq.regexp_replace(\"no_digit_text\", r\"[^\\P{P}-]+\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249474f3-1a8a-4ef8-81f4-97befdbc72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "df = df.dropDuplicates()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04c4b9-cd77-4977-a83c-0173ef2874f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "import pyspark.ml.feature as ft\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = ft.RegexTokenizer(inputCol=\"text_ready\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "stopwordsRemover = ft.StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "ngram = ft.NGram(n=3, inputCol=\"filtered\", outputCol=\"nGrams\")\n",
    "\n",
    "\n",
    "# bag of words count\n",
    "countVectors = ft.CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "word2Vec = ft.Word2Vec(vectorSize=10, seed=42, inputCol=\"filtered\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9771b9-4215-4676-ac71-efca4f579ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "#crete pipeline\n",
    "pipeline_w = Pipeline(stages=[regexTokenizer, stopwordsRemover, word2Vec])\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit_w = pipeline_w.fit(df)\n",
    "pipelineFit = pipeline.fit(df)\n",
    "dataset_w = pipelineFit_w.transform(df)\n",
    "dataset = pipelineFit.transform(df)\n",
    "dataset.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f92f62-7602-4fa1-81bd-5dd5104507be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_w = dataset_w[\"text_ready\", \"features\", \"label\"]\n",
    "dataset = dataset[\"text_ready\", \"features\", \"label\"]\n",
    "# set seed for reproducibility\n",
    "(trainingData_w, testData_w) = dataset_w.randomSplit([0.7, 0.3], seed = 100)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308a50d-91b5-49dd-8934-09d96fbf23cf",
   "metadata": {},
   "source": [
    "## LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7043f4-0ab1-4ef1-b554-5644dd38ab42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData_w)\n",
    "lrw_predictions = lrModel.transform(testData_w)\n",
    "lrw_predictions.filter(lrw_predictions['prediction'] == 0) \\\n",
    "    .select(\"text_ready\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd379e-fd42-4e9e-acea-8af46115f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lrw_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "lr_ev = lrw_evaluator.evaluate(lrw_predictions)\n",
    "print(\"Logistic Regression Accuracy: \\n\" + str(lr_ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f94387-3246-40cf-9bb6-4955cddcb048",
   "metadata": {},
   "source": [
    "## Logistic Regression with word2vec percision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf7e7e-a64d-4552-b781-b994ef92f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array(lrw_predictions.select(\"label\").collect())\n",
    "y_pred = np.array(lrw_predictions.select(\"prediction\").collect())\n",
    "\n",
    "\n",
    "print(\"Logistic Regression model with word2vec Recall score: {}\".format(recall_score(y_true,y_pred)))\n",
    "print(\"Logistic Regression model with word2ve Precision score: {}\".format(precision_score(y_true,y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152f7e3-6b91-4d8b-98ba-02299b0ce157",
   "metadata": {},
   "source": [
    "## Naive Bayes model with countVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a6419-60f3-49e6-b778-aa9db449b0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "nb_predictions = model.transform(testData)\n",
    "nb_predictions.filter(nb_predictions['prediction'] == 0) \\\n",
    "    .select(\"text_ready\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36399b-fda5-4d66-a6cf-5455c965641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "nb_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "nb_ev = nb_evaluator.evaluate(nb_predictions)\n",
    "print(\"Naive Bayes model Accuracy: \\n\" + str(nb_ev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e5fe2-ea4b-4204-b09c-1c6dd0f473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array(nb_predictions.select(\"label\").collect())\n",
    "y_pred = np.array(nb_predictions.select(\"prediction\").collect())\n",
    "\n",
    "\n",
    "print(\"Naive Bayes model with countVec Recall score: {}\".format(recall_score(y_true,y_pred)))\n",
    "print(\"Naive Bayes model with countVec Precision score: {}\".format(precision_score(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623698d6-b768-408f-8a59-505dd5ef99fe",
   "metadata": {},
   "source": [
    "## Random Forest Classifier with countVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde73471-086d-49e2-a66b-63926190cf0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "rf_predictions = rfModel.transform(testData)\n",
    "rf_predictions.filter(rf_predictions['prediction'] == 0) \\\n",
    "    .select(\"text_ready\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794a6fc-01a5-4e74-973e-2ee43a6fb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "rf_ev = evaluator.evaluate(rf_predictions)\n",
    "print(\"Random Forest model Accuracy: \\n\" + str(rf_ev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebaae00-911e-45e3-b19d-5691d2f735cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array(rf_predictions.select(\"label\").collect())\n",
    "y_pred = np.array(rf_predictions.select(\"prediction\").collect())\n",
    "\n",
    "\n",
    "print(\"Random Forest model with countVec Recall score: {}\".format(recall_score(y_true,y_pred)))\n",
    "print(\"Random Forest model with countVec Precision score: {}\".format(precision_score(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2712c-9602-4281-8236-8d767949088b",
   "metadata": {},
   "source": [
    "## Random Forest Classifier with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca73771-ffab-425f-8cfa-262a17525504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData_w)\n",
    "rf_predictions_w = rfModel.transform(testData_w)\n",
    "rf_predictions_w.filter(rf_predictions_w['prediction'] == 0) \\\n",
    "    .select(\"text_ready\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48110383-1e3d-431a-867a-b00acc9a5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "rf_ev_w = evaluator.evaluate(rf_predictions_w)\n",
    "print(\"Random Forest with word2vec model Accuracy: \\n\" + str(rf_ev_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7a145-cd9c-4e0c-b578-d0fcdddf3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(rf_predictions_w.select(\"label\").collect())\n",
    "y_pred = np.array(rf_predictions_w.select(\"prediction\").collect())\n",
    "\n",
    "\n",
    "print(\"Random Forest model with w2v Recall score: {}\".format(recall_score(y_true,y_pred)))\n",
    "print(\"Random Forest model with w2v Precision score: {}\".format(precision_score(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6c796-2a9f-4dd6-9f69-3e57dae5a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f9b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
