{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda40ee7-a987-4a68-b601-46069ccc33fa",
   "metadata": {},
   "source": [
    "# Create spark session and spark dataframe from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564e816f-d19a-4d1c-981e-a8bf02b063b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/08 17:37:21 WARN Utils: Your hostname, YasamanEms-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.250 instead (on interface en0)\n",
      "21/12/08 17:37:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/12/08 17:37:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/08 17:37:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "|    reviewerID|      asin|       reviewerName|helpful|          reviewText|overall|       summary|unixReviewTime| reviewTime|\n",
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "|A240ORQ2LF9LUI|0077613252|         Michelle W|   null|The materials arr...|    4.0|Material Great|    1394496000|03 11, 2014|\n",
      "|A1YCCU0YRLS0FE|0077613252|Rosalind White Ames|   null|I am really enjoy...|    4.0|        Health|    1393113600|02 23, 2014|\n",
      "+--------------+----------+-------------------+-------+--------------------+-------+--------------+--------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "      StructField(\"reviewerID\",StringType(),True),\n",
    "      StructField(\"asin\",StringType(),True),\n",
    "      StructField(\"reviewerName\",StringType(),True),\n",
    "      StructField(\"helpful\",StringType(),True),\n",
    "      StructField(\"reviewText\",StringType(),True),\n",
    "      StructField(\"overall\",StringType(),True),\n",
    "      StructField(\"summary\",StringType(),True),\n",
    "      StructField(\"unixReviewTime\",StringType(),True),\n",
    "      StructField(\"reviewTime\",StringType(),True)\n",
    "  ])\n",
    "df = spark.read.schema(schema).json('data/Software.json')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ec4c2-f40b-42c2-9206-d06f22c06f01",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa3fe02-571d-46a5-b90c-551be8298c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "df = df.selectExpr(\"cast(reviewText as string) reviewText\",\n",
    "                    \"cast(overall as int) overall\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6572b7-3cc9-4fb2-be67-2222da2ba23f",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f2dd0a-889a-46e4-8086-9d4b7cc156c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any rows contain null\n",
    "df = df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0657b-0431-45d0-b3a1-93ab0c5e98bc",
   "metadata": {},
   "source": [
    "### Checking the number of records for each overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b59675-3fbe-4256-96f1-bd1e3d3ed667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|count(overall)|overall|\n",
      "+--------------+-------+\n",
      "|        102542|      1|\n",
      "|         39394|      3|\n",
      "|        212399|      5|\n",
      "|         73590|      4|\n",
      "|         31445|      2|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd3acb-1106-4c65-8d32-15a25b7d6186",
   "metadata": {},
   "source": [
    "### Filter rating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaebf12-a23a-4112-b07a-b8f15ba973f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(\"overall<6 and overall!=3\")\n",
    "df = df.where(\"overall>0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe8318-53a6-443a-86e2-1927d063e196",
   "metadata": {},
   "source": [
    "### Cheking scores after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c32960-49be-422f-818a-eff7b8b6805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|count(overall)|overall|\n",
      "+--------------+-------+\n",
      "|        102542|      1|\n",
      "|        212399|      5|\n",
      "|         73590|      4|\n",
      "|         31445|      2|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "# creating sparksession and giving app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((overall)),overall from df group by overall\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ef4ce-4afb-44b2-8e13-afde1ee675cc",
   "metadata": {},
   "source": [
    "### Bucketizing overall scores to two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd4f879-c2b0-403b-aef4-39fbf90fdeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+\n",
      "|          reviewText|overall|category|\n",
      "+--------------------+-------+--------+\n",
      "|The materials arr...|      4|     1.0|\n",
      "|I am really enjoy...|      4|     1.0|\n",
      "|IF YOU ARE TAKING...|      1|     0.0|\n",
      "|I have used Learn...|      5|     1.0|\n",
      "|Strong backgroung...|      4|     1.0|\n",
      "|i got this book o...|      5|     1.0|\n",
      "|I was very happy ...|      5|     1.0|\n",
      "|Recieved in a tim...|      5|     1.0|\n",
      "|Maybe it's just m...|      2|     0.0|\n",
      "|This was the text...|      5|     1.0|\n",
      "|Not worth the pri...|      2|     0.0|\n",
      "|I love how this b...|      4|     1.0|\n",
      "|Great on the deli...|      5|     1.0|\n",
      "|The book was deli...|      5|     1.0|\n",
      "|Required to buy t...|      2|     0.0|\n",
      "|Didn't help me mu...|      1|     0.0|\n",
      "|Disappointing tex...|      1|     0.0|\n",
      "|This book provide...|      4|     1.0|\n",
      "|I've been using D...|      4|     1.0|\n",
      "|The demo is done ...|      4|     1.0|\n",
      "+--------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketizer = Bucketizer(splits=[ 1, 4, 5 ],inputCol=\"overall\", outputCol=\"category\")\n",
    "df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a5fe5-3fe7-4150-b06d-cc3152128ccd",
   "metadata": {},
   "source": [
    "### Replacing regex and convert sentiments to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314bb1ce-a67d-4d88-bf93-5bd282a3af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sq\n",
    "from pyspark.sql.functions import lower, col\n",
    "df = df.select(\"*\", lower(col('reviewText')).alias(\"lower_text\"))\n",
    "df = df.withColumn(\"no_line_text\", sq.regexp_replace(\"lower_text\", r\"\\n\", \" \"))\n",
    "df = df.withColumn(\"no_digit_text\", sq.regexp_replace(\"no_line_text\", r\"[0-9]\", \" \"))\n",
    "df = df.withColumn(\"text_ready\", sq.regexp_replace(\"no_digit_text\", r\"[^\\P{P}-]+\", \" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02bce4-7397-44a8-9f78-116b2f427f39",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9319f35b-2213-4333-94cd-975358e59a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " # of records including duplicates:  419976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n # of records including duplicates:  \" + str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4c1e39-8f9c-4aa7-a89f-986eefb3d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4deb371e-c08a-46cf-80e9-ec0c2a40d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================================>        (170 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " # of records after drop duplicates:  385876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n # of records after drop duplicates:  \" + str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672e21c-6744-4bdc-89a1-8df539a84294",
   "metadata": {},
   "source": [
    "### Shuffling records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac30c5d-afa4-4d2e-a7d2-af71305e034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "#shuffling data\n",
    "df = df.orderBy(rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afa8c4-72c2-4872-ae54-cb55a1d78beb",
   "metadata": {},
   "source": [
    "### Formating labels as fasttext requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a21d0d-5185-4328-91e9-ea0446745eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "t = {0.0:\"__label__negative\",1.0:\"__label__positive\"}\n",
    "udf_cat = udf(lambda x: t[x], StringType())\n",
    "\n",
    "df = df.withColumn(\"label\", udf_cat(\"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d05a75-13d9-49eb-b54b-2e95d2739cbf",
   "metadata": {},
   "source": [
    "### Cheking count of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc7d841-80cb-4c75-aee0-2d3cca08deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|count(label)|            label|\n",
      "+------------+-----------------+\n",
      "|      256690|__label__positive|\n",
      "|      129186|__label__negative|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating sparksession and giving app name\n",
    "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
    "# creating a temporary view of\n",
    "# Dataframe and storing it into df2\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# using the SQL query to count all\n",
    "# distinct records and display the\n",
    "# count on the screen\n",
    "spark.sql(\"select count((label)),label from df group by label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72655d5d-366f-485a-9985-cd6e38adf650",
   "metadata": {},
   "source": [
    "# Split Data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2307d2e8-d387-4c0c-9817-8c7e5d2c3b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|          reviewText|overall|category|          lower_text|        no_line_text|       no_digit_text|          text_ready|            label|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|'HGTV Home Design...|      4|     1.0|'hgtv home design...|'hgtv home design...|'hgtv home design...| hgtv home design...|__label__positive|\n",
      "|* Important Note ...|      5|     1.0|* important note ...|* important note ...|* important note ...|  important note ...|__label__positive|\n",
      "|-The program caus...|      1|     0.0|-the program caus...|-the program caus...|-the program caus...|-the program caus...|__label__negative|\n",
      "|..This is a piece...|      1|     0.0|..this is a piece...|..this is a piece...|..this is a piece...| this is a piece ...|__label__negative|\n",
      "|1st time reviewer...|      1|     0.0|1st time reviewer...|1st time reviewer...| st time reviewer...| st time reviewer...|__label__negative|\n",
      "|2010: I switched ...|      5|     1.0|2010: i switched ...|2010: i switched ...|    : i switched ...|      i switched ...|__label__positive|\n",
      "|2014 is have a co...|      1|     0.0|2014 is have a co...|2014 is have a co...|     is have a co...|     is have a co...|__label__negative|\n",
      "|3 licenses for at...|      4|     1.0|3 licenses for at...|3 licenses for at...|  licenses for at...|  licenses for at...|__label__positive|\n",
      "|360 is the best p...|      5|     1.0|360 is the best p...|360 is the best p...|    is the best p...|    is the best p...|__label__positive|\n",
      "|<a data-hook=\"pro...|      2|     0.0|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook= pro...|__label__negative|\n",
      "|<a data-hook=\"pro...|      5|     1.0|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook= pro...|__label__positive|\n",
      "|A Very Reliable a...|      5|     1.0|a very reliable a...|a very reliable a...|a very reliable a...|a very reliable a...|__label__positive|\n",
      "|A full installati...|      4|     1.0|a full installati...|a full installati...|a full installati...|a full installati...|__label__positive|\n",
      "|A good inexpensiv...|      5|     1.0|a good inexpensiv...|a good inexpensiv...|a good inexpensiv...|a good inexpensiv...|__label__positive|\n",
      "|A lot about this ...|      4|     1.0|a lot about this ...|a lot about this ...|a lot about this ...|a lot about this ...|__label__positive|\n",
      "|A lot going on. V...|      2|     0.0|a lot going on. v...|a lot going on. v...|a lot going on. v...|a lot going on  v...|__label__negative|\n",
      "|A must have, what...|      5|     1.0|a must have, what...|a must have, what...|a must have, what...|a must have  what...|__label__positive|\n",
      "|A previoius revie...|      5|     1.0|a previoius revie...|a previoius revie...|a previoius revie...|a previoius revie...|__label__positive|\n",
      "|About 6 years ago...|      1|     0.0|about 6 years ago...|about 6 years ago...|about   years ago...|about   years ago...|__label__negative|\n",
      "|According to Amaz...|      1|     0.0|according to amaz...|according to amaz...|according to amaz...|according to amaz...|__label__negative|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n",
    "trainingData.show(truncate = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93d4a8-0235-4433-baaf-928eb79e0416",
   "metadata": {},
   "source": [
    "# Concatintating label and asociated sentiment to create text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98743f29-3677-46cb-b069-18e539e1a9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,col,lit\n",
    "\n",
    "train_df = trainingData.select(concat(trainingData.label,lit(\" \"),trainingData.text_ready).alias(\"all\"))\n",
    "\n",
    "test_df = testData.select(concat(testData.label,lit(\" \"),testData.text_ready).alias(\"all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c555cc-9f28-4482-bc44-e25597cb4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|                                                         all|\n",
      "+------------------------------------------------------------+\n",
      "|__label__positive  hgtv home design   remodeling suite   ...|\n",
      "|__label__positive   important note at bottom    i own the...|\n",
      "|__label__negative -the program caused my computer to shut...|\n",
      "|__label__negative  this is a piece of junk   i don t have...|\n",
      "|__label__negative  st time reviewer here  i purchased thi...|\n",
      "|__label__positive       i switched from  turbotax  to  h ...|\n",
      "|__label__negative      is have a communication problem wi...|\n",
      "|__label__positive   licenses for at most $   each  often ...|\n",
      "|        __label__positive     is the best protection  i find|\n",
      "|__label__negative <a data-hook= product-link-linked  clas...|\n",
      "|__label__positive <a data-hook= product-link-linked  clas...|\n",
      "|__label__positive a very reliable and upgraded version of...|\n",
      "|__label__positive a full installation within minutes  a g...|\n",
      "|__label__positive a good inexpensive program  works well ...|\n",
      "|__label__positive a lot about this program is great -- th...|\n",
      "|__label__negative a lot going on  very different from old...|\n",
      "|    __label__positive a must have  what are you waiting for |\n",
      "|__label__positive a previoius reviewer informed us that s...|\n",
      "|__label__negative about   years ago i used ghost     to m...|\n",
      "|__label__negative according to amazon i should be able to...|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b87d85-1208-41be-819e-03501b2ed6aa",
   "metadata": {},
   "source": [
    "## Write datasets to text file as fasttext requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3d82ac-097e-429e-bc19-fe0a608ec51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 446:=================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "textfile = open(\"train_data.txt\", \"w\")\n",
    "i =0\n",
    "for row in list(train_df.toLocalIterator()):\n",
    "    i += 1\n",
    "    textfile.write(row['all'] + \"\\n\")\n",
    "\n",
    "textfile.close()\n",
    "\n",
    "textfile = open(\"test_data.txt\", \"w\")\n",
    "i =0\n",
    "for row in list(test_df.toLocalIterator()):\n",
    "    i += 1\n",
    "    textfile.write(row['all'] + \"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c3daee-9e71-4006-9969-421630fbbfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " number of training records: 270270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 850:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " number of testing records: 115606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n number of training records: \" + str(train_df.count()))\n",
    "print(\"\\n number of testing records: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80812d89-68c7-4e91-959f-b58c6ebaf5d8",
   "metadata": {},
   "source": [
    "# Training fasttext classification model, manully set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cd1b55c-ebdf-47ca-9ce5-a317c049d856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 22M words\n",
      "Number of words:  124203\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 4129179 lr:  0.000000 avg.loss:  0.238139 ETA:   0h 0m 0s 10.4% words/sec/thread: 4416722 lr:  0.089640 avg.loss:  0.282216 ETA:   0h 0m15s 15.4% words/sec/thread: 4373847 lr:  0.084637 avg.loss:  0.272456 ETA:   0h 0m15s4373387 lr:  0.080081 avg.loss:  0.267244 ETA:   0h 0m14s 21.0% words/sec/thread: 4362717 lr:  0.079017 avg.loss:  0.265886 ETA:   0h 0m14s 29.2% words/sec/thread: 4311618 lr:  0.070823 avg.loss:  0.260784 ETA:   0h 0m12s 47.2% words/sec/thread: 4167043 lr:  0.052797 avg.loss:  0.251859 ETA:   0h 0m 9s 54.8% words/sec/thread: 4168602 lr:  0.045209 avg.loss:  0.249266 ETA:   0h 0m 8s 58.0% words/sec/thread: 4167755 lr:  0.041951 avg.loss:  0.248626 ETA:   0h 0m 7s 63.4% words/sec/thread: 4164887 lr:  0.036561 avg.loss:  0.247242 ETA:   0h 0m 6s 75.4% words/sec/thread: 4163943 lr:  0.024630 avg.loss:  0.243705 ETA:   0h 0m 4s 99.4% words/sec/thread: 4148120 lr:  0.000631 avg.loss:  0.238249 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input=\"train_data.txt\", lr=0.1, epoch=25, wordNgrams=1, bucket=200000, dim=10, loss='hs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c13f38-235d-4e5a-b754-c3c526dedf74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training fasttext classification model with auto hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ead94ea6-84b1-4e94-bd18-c7aba9f200f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   10 Best score:  0.935367 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 22M words\n",
      "Number of words:  124203\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1107950 lr:  0.000000 avg.loss:  0.129678 ETA:   0h 0m 0s 30.7% words/sec/thread: 1102968 lr:  0.043083 avg.loss:  0.223019 ETA:   0h 0m17s 61.8% words/sec/thread: 1168993 lr:  0.023771 avg.loss:  0.165925 ETA:   0h 0m 9s 65.9% words/sec/thread: 1167354 lr:  0.021201 avg.loss:  0.161332 ETA:   0h 0m 8s 70.6% words/sec/thread: 1167973 lr:  0.018287 avg.loss:  0.155816 ETA:   0h 0m 7s 71.3% words/sec/thread: 1166427 lr:  0.017820 avg.loss:  0.154981 ETA:   0h 0m 6s 87.8% words/sec/thread: 1146151 lr:  0.007594 avg.loss:  0.139309 ETA:   0h 0m 3s 91.5% words/sec/thread: 1130862 lr:  0.005274 avg.loss:  0.136103 ETA:   0h 0m 2s 92.2% words/sec/thread: 1129596 lr:  0.004839 avg.loss:  0.135424 ETA:   0h 0m 1s 96.9% words/sec/thread: 1114035 lr:  0.001899 avg.loss:  0.131922 ETA:   0h 0m 0s 97.7% words/sec/thread: 1113553 lr:  0.001433 avg.loss:  0.131427 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_auto_hyper = fasttext.train_supervised(input=\"train_data.txt\", autotuneValidationFile='test_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92bffb-686a-41e4-a2cb-bb8e7a4d1ea5",
   "metadata": {},
   "source": [
    "## Computing Percision & Recall for binary classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f79589-b431-4c4f-8979-1842fa8e5126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 856:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+-----------------+\n",
      "|          reviewText|overall|category|          lower_text|        no_line_text|       no_digit_text|          text_ready|            label|       prediction|prediction_auto_m|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+-----------------+\n",
      "|......by some of ...|      5|     1.0|......by some of ...|......by some of ...|......by some of ...| by some of the s...|__label__positive|__label__positive|__label__positive|\n",
      "|4 stars because n...|      4|     1.0|4 stars because n...|4 stars because n...|  stars because n...|  stars because n...|__label__positive|__label__positive|__label__positive|\n",
      "|<a data-hook=\"pro...|      1|     0.0|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook=\"pro...|<a data-hook= pro...|__label__negative|__label__negative|__label__negative|\n",
      "|                A Ok|      4|     1.0|                a ok|                a ok|                a ok|                a ok|__label__positive|__label__positive|__label__positive|\n",
      "|A PEICE OF JUNK.....|      1|     0.0|a peice of junk.....|a peice of junk.....|a peice of junk.....|a peice of junk j...|__label__negative|__label__negative|__label__negative|\n",
      "|A good product fo...|      4|     1.0|a good product fo...|a good product fo...|a good product fo...|a good product fo...|__label__positive|__label__positive|__label__positive|\n",
      "|     A great product|      5|     1.0|     a great product|     a great product|     a great product|     a great product|__label__positive|__label__positive|__label__positive|\n",
      "|AVG Antivirus 201...|      5|     1.0|avg antivirus 201...|avg antivirus 201...|avg antivirus    ...|avg antivirus    ...|__label__positive|__label__positive|__label__positive|\n",
      "|Actually, Ouino S...|      5|     1.0|actually, ouino s...|actually, ouino s...|actually, ouino s...|actually  ouino s...|__label__positive|__label__positive|__label__positive|\n",
      "|After a little in...|      4|     1.0|after a little in...|after a little in...|after a little in...|after a little in...|__label__positive|__label__positive|__label__positive|\n",
      "|After doing a lot...|      1|     0.0|after doing a lot...|after doing a lot...|after doing a lot...|after doing a lot...|__label__negative|__label__negative|__label__negative|\n",
      "|After getting bac...|      1|     0.0|after getting bac...|after getting bac...|after getting bac...|after getting bac...|__label__negative|__label__negative|__label__negative|\n",
      "|After having used...|      1|     0.0|after having used...|after having used...|after having used...|after having used...|__label__negative|__label__negative|__label__negative|\n",
      "|After installing ...|      1|     0.0|after installing ...|after installing ...|after installing ...|after installing ...|__label__negative|__label__negative|__label__negative|\n",
      "|After last year T...|      1|     0.0|after last year t...|after last year t...|after last year t...|after last year t...|__label__negative|__label__negative|__label__negative|\n",
      "|After waiting on ...|      1|     0.0|after waiting on ...|after waiting on ...|after waiting on ...|after waiting on ...|__label__negative|__label__positive|__label__negative|\n",
      "|Alas, the program...|      4|     1.0|alas, the program...|alas, the program...|alas, the program...|alas  the program...|__label__positive|__label__negative|__label__negative|\n",
      "|Allows me to read...|      5|     1.0|allows me to read...|allows me to read...|allows me to read...|allows me to read...|__label__positive|__label__positive|__label__positive|\n",
      "|Allows me to read...|      4|     1.0|allows me to read...|allows me to read...|allows me to read...|allows me to read...|__label__positive|__label__positive|__label__positive|\n",
      "|Always had good l...|      4|     1.0|always had good l...|always had good l...|always had good l...|always had good l...|__label__positive|__label__positive|__label__positive|\n",
      "+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "\n",
    "test = testData.select('text_ready').rdd.flatMap(lambda x: x).collect()\n",
    "pred = model.predict(test)\n",
    "pred_auto = model_auto_hyper.predict(test)\n",
    "d = pred[0][:]\n",
    "d_auto = pred_auto[0][:]\n",
    "testData = testData.repartition(1).withColumn(\n",
    "    \"prediction\", \n",
    "    udf(lambda id: ' '.join(d[id]))(monotonically_increasing_id()))\n",
    "testData = testData.repartition(1).withColumn(\n",
    "    \"prediction_auto_m\", \n",
    "    udf(lambda id: ' '.join(d_auto[id]))(monotonically_increasing_id()))\n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a8d96-7c4a-402d-984c-b2ba3f745ad2",
   "metadata": {},
   "source": [
    "### manual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7fec3a-4557-4fdb-90af-b2b1dac59516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115606, 0.9114751829489819, 0.9114751829489819)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c7a1e00-01c7-4693-b216-14b60c60596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 71988\n",
      "True Negatives: 33384\n",
      "False Positives: 5444\n",
      "False Negatives: 4790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 875:==================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  115606\n",
      "\n",
      " recall 0.9376123368673318\n",
      "\n",
      " precision 0.9296931501188139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tp = testData[(testData.label == \"__label__positive\") & (testData.prediction == \"__label__positive\")].count()\n",
    "tn = testData[(testData.label == \"__label__negative\") & (testData.prediction == \"__label__negative\")].count()\n",
    "fp = testData[(testData.label == \"__label__negative\") & (testData.prediction == \"__label__positive\")].count()\n",
    "fn = testData[(testData.label == \"__label__positive\") & (testData.prediction == \"__label__negative\")].count()\n",
    "print (\"True Positives:\", str(tp))\n",
    "print (\"True Negatives:\", str(tn))\n",
    "print (\"False Positives:\", str(fp))\n",
    "print (\"False Negatives:\", str(fn))\n",
    "print (\"Total: \", str(testData.count()))\n",
    "\n",
    "r = float(tp)/(tp + fn)\n",
    "print (\"\\n recall\", str(r))\n",
    "\n",
    "p = float(tp) / (tp + fp)\n",
    "print (\"\\n precision\", str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597d359-3dad-45f0-a2f4-fc770a42d623",
   "metadata": {},
   "source": [
    "### auto hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "480c0754-e2bd-4f21-81c8-f86dc16c3aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115606, 0.9352801757694237, 0.9352801757694237)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_hyper.test(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef4b6ebf-4dfa-4560-88b9-12e177de0b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 73072\n",
      "True Negatives: 35052\n",
      "False Positives: 3776\n",
      "False Negatives: 3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 894:=============================================>       (172 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  115606\n",
      "\n",
      " auto_param_recall 0.9517309645992341\n",
      "\n",
      " auto_param_precision 0.950864043306267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tp = testData[(testData.label == \"__label__positive\") & (testData.prediction_auto_m == \"__label__positive\")].count()\n",
    "tn = testData[(testData.label == \"__label__negative\") & (testData.prediction_auto_m == \"__label__negative\")].count()\n",
    "fp = testData[(testData.label == \"__label__negative\") & (testData.prediction_auto_m == \"__label__positive\")].count()\n",
    "fn = testData[(testData.label == \"__label__positive\") & (testData.prediction_auto_m == \"__label__negative\")].count()\n",
    "print (\"True Positives:\", str(tp))\n",
    "print (\"True Negatives:\", str(tn))\n",
    "print (\"False Positives:\", str(fp))\n",
    "print (\"False Negatives:\", str(fn))\n",
    "print (\"Total: \", str(testData.count()))\n",
    "\n",
    "r = float(tp)/(tp + fn)\n",
    "print (\"\\n auto_param_recall\", str(r))\n",
    "\n",
    "p = float(tp) / (tp + fp)\n",
    "print (\"\\n auto_param_precision\", str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058dab2-ce92-4f34-be9a-cd1f9db0e4c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting auto hyperparameters values for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c44f9650-9b7a-45cf-bdcb-3bf012e90b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotuneDuration -> 300\n",
      "autotuneMetric -> f1\n",
      "autotuneModelSize -> \n",
      "autotunePredictions -> 1\n",
      "autotuneValidationFile -> test_data.txt\n",
      "bucket -> 5356303\n",
      "cutoff -> 0\n",
      "dim -> 66\n",
      "dsub -> 2\n",
      "epoch -> 9\n",
      "input -> train_data.txt\n",
      "label -> __label__\n",
      "loss -> loss_name.softmax\n",
      "lr -> 0.062164072811046224\n",
      "lrUpdateRate -> 100\n",
      "maxn -> 0\n",
      "minCount -> 1\n",
      "minCountLabel -> 0\n",
      "minn -> 0\n",
      "model -> model_name.supervised\n",
      "neg -> 5\n",
      "output -> \n",
      "pretrainedVectors -> \n",
      "qnorm -> False\n",
      "qout -> False\n",
      "retrain -> False\n",
      "saveOutput -> False\n",
      "seed -> 0\n",
      "setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7fd1a9449930>>\n",
      "t -> 0.0001\n",
      "thread -> 7\n",
      "verbose -> 2\n",
      "wordNgrams -> 3\n",
      "ws -> 5\n"
     ]
    }
   ],
   "source": [
    "args_obj = model_auto_hyper.f.getArgs()\n",
    "for hparam in dir(args_obj):\n",
    "    if not hparam.startswith('__'):\n",
    "        print(f\"{hparam} -> {getattr(args_obj, hparam)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a634fb-b2fe-4d18-a2ec-a2a194fb0710",
   "metadata": {},
   "source": [
    "## Saving model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa5486d6-6b88-4240-a54d-6e6568c56347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_amazon_sentiments.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7e9a6-7d8c-4b4f-96a7-e0c0d3f19abc",
   "metadata": {},
   "source": [
    "## Predicting signle sentiments to better see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69b95a22-3a59-475e-8973-2023c23b9932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\" worst customer delivery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "063c2ee1-9119-4504-8c69-f6a551ef2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\" terrible quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2e78e49-e642-42d4-8edb-c086c7cc48f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.99960905]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\" very high-quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e64b9548-e065-4025-b071-4f97c6d10b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\" poor toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd8b3259-41b7-44ca-b55e-6426488dcb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.99960595]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\" nice color of many different items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "117c8514-11f7-4190-bce3-cce7cfd6f3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([1.00000501]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"dont hesitate to buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b2d0498-4a57-4918-a395-24d282a2e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.99768275]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"never buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f181345b-4bf0-4c08-898b-9f3582835940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.93805468]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"came to my door tear up and broken but it seems it was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "359bad04-8262-47e3-b175-b61e8acb8bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.75868022]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"came to my door tear up and broken \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e49f2eb-2441-42a0-acc7-d2ab7f159771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.99987578]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"it was awufully good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d74fa336-72bf-4725-92ba-0fff9cbad22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00001001]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"it sucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c85ee-e18d-4cd4-a6b1-829270dda479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
